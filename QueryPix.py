# -*- coding: utf-8 -*-
"""WhisperModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1twkwraHbTexpOUwXQCgWPu-8WQP8NeVs
"""

!pip install -q transformers==4.37.2
!pip install bitsandbytes==0.41.3
!pip install accelerate==0.25.0
!pip install -q git+https://github.com/openai/whisper.git
!pip install -q gradio
!pip install -q gTTS

import torch
from transformers import BitsAndBytesConfig
from transformers import pipeline

quant_config=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

model_id="llava-hf/llava-1.5-7b-hf"

pipe=pipeline(
    "image-to-text",
    model=model_id,
    model_kwargs={"quantization_config":quant_config},


)

pipe

import whisper
import gradio as gr
import time
import warnings
import os
from gtts import gTTS
from PIL import Image

image_path="/content/testimage.jpg"

image=Image.open((image_path))

image

import nltk
from nltk import sent_tokenize

max_new_tokens=250

prompt_instructions="""
Describe the image using as much as possible detail.
You are a helpful AI assistant who is able to answer questions about images
What is the image all about?
Now generate the helpful answer
"""

prompt="User:<image>\n"+prompt_instructions+"\nAssistant:"

outputs=pipe(image,prompt=prompt,generate_kwargs={"max_new_tokens":max_new_tokens})

outputs

nltk.download('punkt')
for sent in sent_tokenize(outputs[0]["generated_text"]):
  print(sent)

warnings.filterwarnings("ignore")

import numpy as np

torch.cuda.is_available()

torch_device="cuda:0" if torch.cuda.is_available() else "cpu"

print(f"Using torch {torch.__version__} ({torch_device})")

import whisper

model=whisper.load_model("medium",device=torch_device)

print(
    f"Model is {'multilingual' if model.is_multilingual else 'English-only'} "
    f"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters."
)

import re
import datetime

##Logger file
tstamp=datetime.datetime.now()
tstamp=str(tstamp).replace(" ","_")
logfile=f"log_{tstamp}.txt"

def writehistory(text):
  with open(logfile,"a",encoding='utf-8') as f:
    f.write(text)
    f.write(text+"\n")
    f.close()

import requests
from PIL import Image

def img2txt(input_text,input_image):
  image=Image.open(input_image)

  writehistory(f"Input text: {input_text} - Type: {type(input_text)} - Dir: {dir(input_text)}")
  if type(input_text) == tuple:
        prompt_instructions = """
      Describe the image using as much as possible detail.
      You are a helpful AI assistant who is able to answer questions about images
      What is the image all about?
      Now generate the helpful answer
      """
  else:
        prompt_instructions = """
        Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:
        """ + input_text

  writehistory(f"prompt_instructions: {prompt_instructions}")
  prompt = "USER: <image>/n" + prompt_instructions + "\nASSISTANT:"
  ouputs=pipe(image,prompt=prompt,generate_kwargs={"max_new_tokens":200})

  if outputs is not None and len(outputs[0]["generated_text"]) > 0:
   match=re.search(r"ASSISTANT:(.*)",outputs[0]["generated_text"])
   if match:
     reply=match.group(1)
   else:
     reply="No response found."
  else:
    reply="No response found."
  return reply

def transcribe(audio):

    # Check if the audio input is None or empty
    if audio is None or audio == '':
        return ('','',None)  # Return empty strings and None audio file

    # language = 'en'

    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    _, probs = model.detect_language(mel)

    options = whisper.DecodingOptions()
    result = whisper.decode(model, mel, options)
    result_text = result.text

    return result_text

def text_to_speech(text, file_path):
    language = 'en'

    audioobj = gTTS(text = text,
                    lang = language,
                    slow = False)

    audioobj.save(file_path)

    return file_path

import locale
locale.getpreferredencoding=lambda: "UTF-8"

!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3

import gradio as gr
import base64
import os


def process_inputs(audio_path, image_path):

    speech_to_text_output = transcribe(audio_path)


    if image_path:
        chatgpt_output = img2txt(speech_to_text_output, image_path)
    else:
        chatgpt_output = "No image provided."


    processed_audio_path = text_to_speech(chatgpt_output, "Temp3.mp3")

    return speech_to_text_output, chatgpt_output, processed_audio_path

iface = gr.Interface(
    fn=process_inputs,
    inputs=[
        gr.Audio(sources=["microphone"], type="filepath"),
        gr.Image(type="filepath")
    ],
    outputs=[
        gr.Textbox(label="Speech to Text"),
        gr.Textbox(label="AI Output"),
        gr.Audio("Temp.mp3")
    ],
    title="Voice Assistant:",
    description="Upload an image and interact via voice input and audio response."
)

iface.launch(debug=True)

